### 概要
- 潜在変数へのdisentanglementはよく行われているが, 現実世界で意味論的な要素が独立しているとは限らない.
- 独立の因子を因果因子に変換するためのcausal layersを持っているVAEを提案.
- Causal VAEと呼ぶ.
- Causal VAEの識別可能性も解析した.
  - 識別可能性: Q(M)をモデルMにおいて計算可能な量とする. あるモデルのクラスMから得られる任意のモデルM1とM2に対して, P_M1(v)=P_M2(v)が成り立つ場合には, いつでもQ(M1)=Q(M2)である時, MにおいてQは識別可能であるという.
  - 学習した生成モデルがある程度までは真のモデルを復元しているということを示すことで解析した. (はぁ？)
- 実験データセット
  - 物理的因果関係を持つ人工画像データ (太陽と振り子と影)
  - CelebA
- 結果, CausalVAEの因果表現は意味論的に解釈可能で, downstreamタスクに関していい結果だった.

#### 情報の流れ
- xはencoderで潜在変数εに変換される.
  - εは多変量正規分布の事前分布p(ε)を持っている.
- εはcausal layerで因果表現zに変換される.
  - zは条件付き分布p(z|u)を持っている.
  - uはラベルなどの追加情報.
- zはdecoderによってxに変換される.


### 3. Method
#### 3.1 Causal Model
xからεは普通にencoder

εからzは, 以下のようなLinear Structural Equation modelsを仮定している.
![103_01](https://github.com/wataoka/papersheet2md/blob/main/images/103_01.png?raw=true)

zはn次元ベクトルで, それぞれが何らかの物理的な意味に対応している.
Azがzの項でもあることから, ziとzjの因果関係なども記述していることがわかる.

#### 3.2 Generative Model
モデルの教師なし学習は識別可能性問題によって不可能.
([1]で議論済み)

この問題に対処するためにiVAE(Khemargem et al., 2019)を参考に, 観測signalとして真の因果コンセプト情報を使用した. (端的にいえばラベルありにした.) 追加的な観測はラベル, ピクセルレベルの観測などであり, uで表される. u_iがi番目のコンセプトである.

下のような生成モデルを考える.
![103_02](https://github.com/wataoka/papersheet2md/blob/main/images/103_02.png?raw=true)

- 属性情報uによって, 以下が生成される.
  - 画像x
  - 意味論を持つ潜在変数z
  - ノイズ潜在変数ε

#### 3.3 Training Method
相変わらずELBOを最適化する. (p(x|u)のELBO)

因果隣接行列Aの最適化は, continuous constraint functionを使用.
(Zheng et al., 2018; Zhu & Chen, 2019; Ng etal., 2019; Yu et al., 2019)
![103_03](https://github.com/wataoka/papersheet2md/blob/main/images/103_03.png?raw=true)

この関数は次のような性質を持つ.
AがDAGを形成する値である <=> H(A)=0
なので, H(A)を正則化項とすればいいのだが, 2乗項も加えると学習がスムーズになる. 

従って, 下が損失関数.
![103_04](https://github.com/wataoka/papersheet2md/blob/main/images/103_04.png?raw=true)


### 5. Experimetns
#### 5.1 Dataset
#### 5.1.1 Synthetic Data
- Pendulum (振り子)
  - 3つのエンティティ
    - 振り子
    - 光
    - 影
  - 4つのコンセプト
    - 振り子の角度
    - 光の角度
    - 影の場所
    - 影の長さ
- Water
  - 2つのエンティティ
    - 水
    - ボール (水の入ったカップの中にある)
  - 2つのコンセプト
    - ボールのサイズ
    - 水の高さ

#### 5.1.2 Benchmark Dataset
CelebA

### 6 Experiments
- 合成データとCelebAで実験した.
- CausalVAEと既存のdisentangle手法と比較した.
- 以下の点に重点を置いている.
  - アルゴリズムが解釈可能な表現を学習できているか
  - 潜在変数への介入の結果が因果系の理解と一致しているか

#### 6.1 Dataset, Baseliens & Metrics
Metrics
![103_05](https://github.com/wataoka/papersheet2md/blob/main/images/103_05.png?raw=true)

評価指標として以下の2つを使用した.
Maximal Information Coefficient (MIC)
Total Information Coefficient (TIC)
どちらも表現とground truth labels of conceptsとの間の相互情報量を示したもの.


MICはいろいろと分割して相互情報量が最大となる値を採用するmetric. (21世紀の相関)

#### 6.2 Intervention experiments
何かしらのコンセプトと対応しているzを介入した結果, どんな画像を出力されるかを観測した. (振り子とCelebA)

振り子に関してもCelebAに関しても, しっかりと介入できてることを画像を見せて示した.

因果行列Aの学習プロセスをヒートマップの流れで表現し, 真の因果行列に収束していっていることを示した.
![103_06](https://github.com/wataoka/papersheet2md/blob/main/images/103_06.png?raw=true)

### reference
[1] Challenging common assumptions ¨ in the unsupervised learning of disentangled representations.

### wataokaのコメント
潜在変数で因果探索を行うところは良いと思うが, 介入の方法については微妙？定性的評価の時点で少し所望の動きができていないところがある. 現時点でどこにもacceptされていないので, 経過を見たいところ. (2020年12月現在でacceptされてないっぽいので, どこかに出してrejectされたかも.)
